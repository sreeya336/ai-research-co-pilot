# -*- coding: utf-8 -*-
"""AI_Research_Co_Pilot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZIrKfik0rsCiGJFPDxTlbS_N_e5S9y7A
"""

!pip install pypdf

!pip install langchain transformers accelerate sentence-transformers faiss-cpu gradio

!pip install -U langchain langchain-community

from langchain_community.document_loaders import PyPDFLoader

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

from langchain.document_loaders import PyPDFLoader
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
import gradio as gr
import tempfile

def load_pdf(filepath):
    loader = PyPDFLoader(filepath)
    return loader.load_and_split()

def get_embedder():
    return SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

class FAISSStore:
    def __init__(self, embedder):
        self.embedder = embedder
        self.index = None
        self.docs = []

    def build_index(self, docs):
        texts = [doc.page_content for doc in docs]
        self.docs = docs
        vectors = self.embedder.encode(texts)
        self.index = faiss.IndexFlatL2(vectors.shape[1])
        self.index.add(np.array(vectors))

    def query(self, question, k=5):
        q_vec = self.embedder.encode([question])
        D, I = self.index.search(np.array(q_vec), k)
        return [self.docs[i] for i in I[0]]

def load_llm(model_name="google/flan-t5-large"):
    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    pipe = pipeline("text2text-generation", model=model, tokenizer=tokenizer)
    return HuggingFacePipeline(pipeline=pipe)

from langchain.vectorstores import FAISS as LangChainFAISS
from langchain.embeddings import HuggingFaceEmbeddings

def get_retrieved_context(store, question):
    texts = [doc.page_content for doc in store.docs]
    metadata = [{"source": f"chunk_{i}"} for i in range(len(store.docs))]

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    langchain_faiss = LangChainFAISS.from_texts(texts, embeddings, metadatas=metadata)
    retriever = langchain_faiss.as_retriever(search_kwargs={"k": 5})

    docs = retriever.get_relevant_documents(question)
    context = "\n\n".join([doc.page_content for doc in docs])
    return context

embedder = get_embedder()
llm = load_llm()

def handle_query(file, question):
    try:
        import tempfile
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp:
            temp.write(open(file.name, "rb").read())
            pdf_path = temp.name

        docs = load_pdf(pdf_path)

        if len(docs) < 5 or sum(len(d.page_content.split()) for d in docs) < 200:
            return "⚠️ That doesn't look like a research paper. Try uploading something more technical?"

        store = FAISSStore(embedder)
        store.build_index(docs)

        # Get context
        context = get_retrieved_context(store, question)

        # Informal prompt
        prompt = f"""Hey! Here's a paper I'm reading. Based on this, could you casually answer the question:

Question: {question}

Paper bits:
{context}

Try to be helpful, clear, and chill. If the answer isn't really in there, just say that politely.
"""

        response = llm(prompt)
        return response or "Hmm, couldn't find a good answer. Try rephrasing?"

    except Exception as e:
        return f"🔥 Error: {str(e)}"

import gradio as gr

# ✅ Professional minimal CSS
professional_css = """
body {
    background-color: #f8f9fa;
    font-family: 'Segoe UI', sans-serif;
}

.gradio-container {
    max-width: 800px;
    margin: auto;
    padding: 20px;
    background-color: white;
    border-radius: 8px;
    box-shadow: 0 0 15px rgba(0, 0, 0, 0.08);
}

h1, h2 {
    text-align: center;
    color: #343a40;
}

textarea, input[type="file"], input[type="text"] {
    border: 1px solid #ced4da;
    border-radius: 4px;
    padding: 10px;
    font-size: 15px;
    background-color: #ffffff;
}

button {
    background-color: #0069d9;
    color: white;
    border: none;
    border-radius: 4px;
    padding: 10px 18px;
    font-size: 15px;
    cursor: pointer;
}

button:hover {
    background-color: #0056b3;
}
"""

# ✅ Build the app layout
with gr.Blocks(css=professional_css) as demo:
    gr.Markdown("## AI Research Co-Pilot")
    gr.Markdown("Upload a research paper and ask questions about it using RAG + Hugging Face.")

    with gr.Row():
        file_input = gr.File(label="Upload Research Paper (PDF)")
        question_input = gr.Textbox(label="Ask a Question", placeholder="e.g. What is the main contribution?")

    output = gr.Textbox(label="Answer", lines=6)
    submit_button = gr.Button("Generate Answer")

    def run_query(file, question):
        return handle_query(file, question)

    submit_button.click(fn=run_query, inputs=[file_input, question_input], outputs=output)

demo.launch()

